{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitxcs224nvenvc3830852de054796a1ac976dcd799bb1",
   "display_name": "Python 3.7.4 64-bit ('xcs224n': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.config(rewrite_asserts=True, magics=True)\n",
    "\n",
    "__file__ = \"sandbox.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sanity_checks import dummy\n",
    "from utils.utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'test_word2vec': {\n",
    "        'currentCenterWord': \"c\",\n",
    "        'windowSize': 3,\n",
    "        'outsideWords': [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"]\n",
    "    },\n",
    "    'test_naivesoftmax': {\n",
    "        'centerWordVec': np.array([-0.27323645, 0.12538062, 0.95374082]).astype(float),\n",
    "        'outsideWordIdx': 3,\n",
    "        'outsideVectors': np.array([[-0.6831809, -0.04200519, 0.72904007],\n",
    "                                    [0.18289107, 0.76098587, -0.62245591],\n",
    "                                    [-0.61517874, 0.5147624, -0.59713884],\n",
    "                                    [-0.33867074, -0.80966534, -0.47931635],\n",
    "                                    [-0.52629529, -0.78190408, 0.33412466]]).astype(float)\n",
    "\n",
    "    },\n",
    "    'test_sigmoid': {\n",
    "        'x': np.array([-0.46612273, -0.87671855, 0.54822123, -0.36443576, -0.87671855, 0.33688521\n",
    "                          , -0.87671855, 0.33688521, -0.36443576, -0.36443576, 0.54822123]).astype(float)\n",
    "    }\n",
    "}\n",
    "\n",
    "outputs = {\n",
    "    'test_word2vec': {\n",
    "        'loss': 11.16610900153398,\n",
    "        'dj_dv': np.array(\n",
    "            [[0., 0., 0.],\n",
    "             [0., 0., 0.],\n",
    "             [-1.26947339, -1.36873189, 2.45158957],\n",
    "             [0., 0., 0.],\n",
    "             [0., 0., 0.]]).astype(float),\n",
    "        'dj_du': np.array(\n",
    "            [[-0.41045956, 0.18834851, 1.43272264],\n",
    "             [0.38202831, -0.17530219, -1.33348241],\n",
    "             [0.07009355, -0.03216399, -0.24466386],\n",
    "             [0.09472154, -0.04346509, -0.33062865],\n",
    "             [-0.13638384, 0.06258276, 0.47605228]]).astype(float)\n",
    "\n",
    "    },\n",
    "    'test_naivesoftmax': {\n",
    "        'loss': 2.217424877675181,\n",
    "        'dj_dvc': np.array([-0.17249875, 0.64873661, 0.67821423]).astype(float),\n",
    "        'dj_du': np.array([[-0.11394933, 0.05228819, 0.39774391],\n",
    "                           [-0.02740743, 0.01257651, 0.09566654],\n",
    "                           [-0.03385715, 0.01553611, 0.11817949],\n",
    "                           [0.24348396, -0.11172803, -0.84988879],\n",
    "                           [-0.06827005, 0.03132723, 0.23829885]]).astype(float)\n",
    "    },\n",
    "    'test_sigmoid': {\n",
    "        's': np.array(\n",
    "            [0.38553435, 0.29385824, 0.63372281, 0.40988622, 0.29385824, 0.5834337, 0.29385824, 0.5834337, 0.40988622,\n",
    "             0.40988622, 0.63372281]).astype(float),\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    s = np.exp(x)/(np.exp(x)+1)\n",
    "\n",
    "    return s\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sigmoid_data = {\n",
    "        'x': np.array([-0.46612273, -0.87671855, 0.54822123, -0.36443576, -0.87671855, 0.33688521\n",
    "                          , -0.87671855, 0.33688521, -0.36443576, -0.36443576, 0.54822123]).astype(float),\n",
    "        's': np.array(\n",
    "            [0.38553435, 0.29385824, 0.63372281, 0.40988622, 0.29385824, 0.5834337, 0.29385824, 0.5834337, 0.40988622,\n",
    "             0.40988622, 0.63372281]).astype(float),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Cell magic `%%run_pytest[clean]` not found.\n"
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_sigmoid():\n",
    "\n",
    "    test_sigmoid_data = {\n",
    "        'x': np.array([-0.46612273, -0.87671855, 0.54822123, -0.36443576, -0.87671855, 0.33688521\n",
    "                          , -0.87671855, 0.33688521, -0.36443576, -0.36443576, 0.54822123]).astype(float),\n",
    "        's': np.array(\n",
    "            [0.38553435, 0.29385824, 0.63372281, 0.40988622, 0.29385824, 0.5834337, 0.29385824, 0.5834337, 0.40988622,\n",
    "             0.40988622, 0.63372281]).astype(float),\n",
    "    }\n",
    "    assert np.allclose(test_sigmoid_data['s'], sigmoid(test_sigmoid_data['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "        centerWordVec,\n",
    "        outsideWordIdx,\n",
    "        outsideVectors,\n",
    "        dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Implement the naive softmax loss and gradients between a center word's \n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models.\n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors (rows of matrix) for all words in vocab\n",
    "                      (U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    (dJ / dU)\n",
    "                    \n",
    "    Note:\n",
    "     we usually use column vector convention (i.e., vectors are in column form) for vectors in matrix U and V (in the handout)\n",
    "     but for ease of implementation/programming we usually use row vectors (representing vectors in row form).\n",
    "    \"\"\"\n",
    "\n",
    "    ### Please use the provided softmax function (imported earlier in this file)\n",
    "    ### This numerically stable implementation helps you avoid issues pertaining\n",
    "    ### to integer overflow.\n",
    "\n",
    "    # Get word probability distribution with respect to v_c\n",
    "    prob = np.dot(centerWordVec, outsideVectors.T)\n",
    "    y_hat = softmax(prob)\n",
    "\n",
    "    # This will also be the change in weights\n",
    "    delta = y_hat.copy()\n",
    "    delta[outsideWordIdx] -= 1\n",
    "\n",
    "    # Get context word and calculate its naive softmax loss\n",
    "    loss = -np.log(y_hat[outsideWordIdx])\n",
    "    \n",
    "    ### Gradients \n",
    "    # Center word gradient\n",
    "\n",
    "    gradCenterVec = np.dot(delta, outsideVectors)\n",
    "    \n",
    "    # Outside word gradient\n",
    "    # Cf. outer product of matrix x 'column' vector \n",
    "    gradOutsideVecs = np.dot(delta[:, np.newaxis], centerWordVec[np.newaxis, :])\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "============================= test session starts ==============================\nplatform linux -- Python 3.7.4, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 -- /home/ray/.pyenv/versions/xcs224n/bin/python\ncachedir: .pytest_cache\nrootdir: /home/ray/XCS224N/Assignment_2/XCS224N-A2\ncollecting ...collected 1 item\n\nsandbox.py::test_naivesoftmax <- <ipython-input-47-997c8349667a>FAILED  [100%]\n\n=================================== FAILURES ===================================\n______________________________ test_naivesoftmax _______________________________\n\n    def test_naivesoftmax():\n        loss, dj_dv, dj_du = naiveSoftmaxLossAndGradient(\n                inputs['test_naivesoftmax']['centerWordVec'],\n                inputs['test_naivesoftmax']['outsideWordIdx'],\n                inputs['test_naivesoftmax']['outsideVectors'],\n                dataset\n            )\n        assert np.allclose(loss, outputs['test_naivesoftmax']['loss'])\n>       assert np.allclose(dj_dv, outputs['test_naivesoftmax']['dj_dvc'])\n\n<ipython-input-47-997c8349667a>:34:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n<__array_function__ internals>:6: in allclose\n    ???\n../../../.pyenv/versions/xcs224n/lib/python3.7/site-packages/numpy/core/numeric.py:2159: in allclose\n    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n<__array_function__ internals>:6: in isclose\n    ???\n../../../.pyenv/versions/xcs224n/lib/python3.7/site-packages/numpy/core/numeric.py:2260: in isclose\n    return within_tol(x, y, atol, rtol)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nx = array([ 0.87671856, -0.54822124, -0.3368852 , -0.46612272,  0.36443577])\ny = array([-0.17249875,  0.64873661,  0.67821423]), atol = 1e-08, rtol = 1e-05\n\n    def within_tol(x, y, atol, rtol):\n        with errstate(invalid='ignore'):\n>           return less_equal(abs(x-y), atol + rtol * abs(y))\nE           ValueError: operands could not be broadcast together with shapes (5,) (3,)\n\n../../../.pyenv/versions/xcs224n/lib/python3.7/site-packages/numpy/core/numeric.py:2246: ValueError\n=========================== short test summary info ============================\nFAILED sandbox.py::test_naivesoftmax - ValueError: operands could not be broa...\n============================== 1 failed in 0.08s ===============================\n"
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "dataset, dummy_vectors, dummy_tokens = dummy()\n",
    "\n",
    "inputs = {\n",
    "        'test_naivesoftmax': {\n",
    "        'centerWordVec': np.array([-0.27323645, 0.12538062, 0.95374082]).astype(float),\n",
    "        'outsideWordIdx': 3,\n",
    "        'outsideVectors': np.array([[-0.6831809, -0.04200519, 0.72904007],\n",
    "                                    [0.18289107, 0.76098587, -0.62245591],\n",
    "                                    [-0.61517874, 0.5147624, -0.59713884],\n",
    "                                    [-0.33867074, -0.80966534, -0.47931635],\n",
    "                                    [-0.52629529, -0.78190408, 0.33412466]]).astype(float)\n",
    "}\n",
    "}\n",
    "outputs = {'test_naivesoftmax': {\n",
    "        'loss': 2.217424877675181,\n",
    "        'dj_dvc': np.array([-0.17249875, 0.64873661, 0.67821423]).astype(float),\n",
    "        'dj_du': np.array([[-0.11394933, 0.05228819, 0.39774391],\n",
    "                           [-0.02740743, 0.01257651, 0.09566654],\n",
    "                           [-0.03385715, 0.01553611, 0.11817949],\n",
    "                           [0.24348396, -0.11172803, -0.84988879],\n",
    "                           [-0.06827005, 0.03132723, 0.23829885]]).astype(float)\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "def test_naivesoftmax():\n",
    "    loss, dj_dv, dj_du = naiveSoftmaxLossAndGradient(\n",
    "            inputs['test_naivesoftmax']['centerWordVec'],\n",
    "            inputs['test_naivesoftmax']['outsideWordIdx'],\n",
    "            inputs['test_naivesoftmax']['outsideVectors'],\n",
    "            dataset\n",
    "        )\n",
    "    assert np.allclose(loss, outputs['test_naivesoftmax']['loss'])\n",
    "    assert np.allclose(dj_dv, outputs['test_naivesoftmax']['dj_dvc'])\n",
    "    assert np.allclose(dj_du, outputs['test_naivesoftmax']['dj_du'])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}